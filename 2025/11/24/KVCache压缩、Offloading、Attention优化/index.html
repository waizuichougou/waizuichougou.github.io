

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="wyz">
  <meta name="keywords" content="">
  
    <meta name="description" content="KV Cache 相关研究笔记一、KV Cache 压缩（稀疏、量化、驱逐与层间压缩）该领域论文专注于减少 KV Cache 在 GPU 显存中的运行时 (run-time) 占用大小，或减少其传输时 (transmission-time) 的大小。 1. Zhang 等 - 2023 - H2O: Heavy-Hitter Oracle for Efficient Generative Infe">
<meta property="og:type" content="article">
<meta property="og:title" content="KVCache压缩、Offloading、Attention优化论文总结">
<meta property="og:url" content="http://example.com/2025/11/24/KVCache%E5%8E%8B%E7%BC%A9%E3%80%81Offloading%E3%80%81Attention%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="歪嘴臭狗的狗窝">
<meta property="og:description" content="KV Cache 相关研究笔记一、KV Cache 压缩（稀疏、量化、驱逐与层间压缩）该领域论文专注于减少 KV Cache 在 GPU 显存中的运行时 (run-time) 占用大小，或减少其传输时 (transmission-time) 的大小。 1. Zhang 等 - 2023 - H2O: Heavy-Hitter Oracle for Efficient Generative Infe">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-24T06:19:52.000Z">
<meta property="article:modified_time" content="2025-11-24T13:27:15.637Z">
<meta property="article:author" content="wyz">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>KVCache压缩、Offloading、Attention优化论文总结 - 歪嘴臭狗的狗窝</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="KVCache压缩、Offloading、Attention优化论文总结"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-11-24 14:19" pubdate>
          2025年11月24日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          63 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">KVCache压缩、Offloading、Attention优化论文总结</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="KV-Cache-相关研究笔记"><a href="#KV-Cache-相关研究笔记" class="headerlink" title="KV Cache 相关研究笔记"></a>KV Cache 相关研究笔记</h1><h2 id="一、KV-Cache-压缩（稀疏、量化、驱逐与层间压缩）"><a href="#一、KV-Cache-压缩（稀疏、量化、驱逐与层间压缩）" class="headerlink" title="一、KV Cache 压缩（稀疏、量化、驱逐与层间压缩）"></a>一、KV Cache 压缩（稀疏、量化、驱逐与层间压缩）</h2><p>该领域论文专注于减少 KV Cache 在 GPU 显存中的运行时 (run-time) 占用大小，或减少其传输时 (transmission-time) 的大小。</p>
<h3 id="1-Zhang-等-2023-H2O-Heavy-Hitter-Oracle-for-Efficient-Generative-Inference-of-Large-Language-Models-H2O"><a href="#1-Zhang-等-2023-H2O-Heavy-Hitter-Oracle-for-Efficient-Generative-Inference-of-Large-Language-Models-H2O" class="headerlink" title="1. Zhang 等 - 2023 - H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models (H2O)"></a>1. Zhang 等 - 2023 - H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models (H2O)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>问题: 完整 KV Cache 占用巨大显存。</li>
<li>观察: 注意力矩阵具有高度稀疏性（&gt;95%）；一小部分 token（H₂）贡献了绝大部分注意力价值，且与文本共现频率强相关。</li>
<li>方法: 提出 H2O (Heavy-Hitter Oracle)，这是一种 KV Cache 驱逐策略。它不再保留所有的 KV，而是只保留一小部分 H₂ token 和一部分最近(recent)的 token。</li>
<li>优势: H₂ 的确定基于低成本的贪心算法（累加注意力分数），无需预知未来。</li>
</ul>
<h3 id="2-Yang-等-2024-PyramidInfer-Pyramid-KV-Cache-Compression-for-High-throughput-LLM-Inference-PyramidInfer"><a href="#2-Yang-等-2024-PyramidInfer-Pyramid-KV-Cache-Compression-for-High-throughput-LLM-Inference-PyramidInfer" class="headerlink" title="2. Yang 等 - 2024 - PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference (PyramidInfer)"></a>2. Yang 等 - 2024 - PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference (PyramidInfer)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (H2O): H2O 等方法是在 KV Cache 生成后再进行压缩或驱逐，但没有解决预填充 (prefill) 阶段本身的巨大显存消耗。</li>
<li>观察 1 (ICR 假说): 并非所有 token 都需要计算 KV Cache。关键 token（Pivotal Context, PVC）的数量随层深度的增加而减少（即深层更冗余）。</li>
<li>观察 2 (RAC 假说): “最近”的 token 在注意力权重上表现出一致性 (Recent Attention Consistency)，使其能作为”预言机” (oracle) 提前选择出 PVC。</li>
<li>方法: 提出 PyramidInfer，它在预填充阶段就只计算那些被预测为 PVC 的 token 的 KV Cache，而跳过非 PVC token 的计算。由于 PVC 数量逐层递减，KV Cache 形态如“金字塔”。</li>
</ul>
<h3 id="3-Dong-等-2024-Get-More-with-LESS-Synthesizing-Recurrence-with-KV-Cache-Compression-for-Efficient-LLM-Inference-LESS"><a href="#3-Dong-等-2024-Get-More-with-LESS-Synthesizing-Recurrence-with-KV-Cache-Compression-for-Efficient-LLM-Inference-LESS" class="headerlink" title="3. Dong 等 - 2024 - Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference (LESS)"></a>3. Dong 等 - 2024 - Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference (LESS)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (稀疏策略, 如 H2O): H2O 这类基于驱逐的稀疏策略会不可逆地丢弃 token，损害需要回忆长上下文的任务性能。</li>
<li>方法: 提出 LESS，一种混合方法，它结合了稀疏 KV 策略和一个恒定大小的低秩缓存 (constant-sized low-rank cache)。</li>
<li>核心思想: 这个低秩缓存通过学习，旨在近似存储那些被稀疏策略丢弃的信息（即全注意力输出与稀疏注意力输出之间的残差）。</li>
<li>机制: 低秩缓存通过递归更新（类似 RNN）来吸收新丢弃的 KV 对信息，而不是拼接，从而保持恒定大小。</li>
</ul>
<h3 id="4-Liu-等-2024-CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving-CacheGen"><a href="#4-Liu-等-2024-CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving-CacheGen" class="headerlink" title="4. Liu 等 - 2024 - CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving (CacheGen)"></a>4. Liu 等 - 2024 - CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving (CacheGen)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>问题: 专注于一个不同的问题——当 KV Cache 需要跨网络传输时（例如从远程存储加载），其巨大的体积导致高网络延迟。</li>
<li>方法: 提出 CacheGen，一个用于快速上下文加载的模块。它不保留 KV Cache 的张量形态，而是使用自定义的张量编码器将其压缩为紧凑的比特流。</li>
<li>编码器特性: 编码器利用了 KV Cache 的分布特性：<ol>
<li>Token 局部性: 相邻 token 的 KV 值相似，因此编码其增量 (delta)。</li>
<li>层敏感性: 浅层对量化损失更敏感，深层则不那么敏感，因此采用分层量化。</li>
<li>分布特性: 按层和通道分组进行算术编码。</li>
</ol>
</li>
<li>自适应流式传输: CacheGen 能够根据可用网络带宽动态调整压缩级别（即量化程度），以在满足 SLO 的同时保持生成质量。</li>
</ul>
<h3 id="5-Yang-等-2024-KVSharer-Efficient-Inference-via-Layer-Wise-Dissimilar-KV-Cache-Sharing-KVSharer"><a href="#5-Yang-等-2024-KVSharer-Efficient-Inference-via-Layer-Wise-Dissimilar-KV-Cache-Sharing-KVSharer" class="headerlink" title="5. Yang 等 - 2024 - KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing (KVSharer)"></a>5. Yang 等 - 2024 - KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing (KVSharer)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>问题: 探索新的压缩维度——层间压缩（或称深度维度压缩），且无需重新训练。</li>
<li>反直觉发现: 共享不相似 (dissimilar) 的层级 KV Cache（根据欧氏距离衡量）比共享相似的更能保持模型性能。</li>
<li>方法: 提出 KVSharer，一种即插即用的层级 KV Cache 共享方法。</li>
<li>搜索策略: <ol>
<li>计算任意两层 KV Cache 展平后的欧氏距离；</li>
<li>按距离降序（即最不相似的优先）排序；</li>
<li>依次尝试共享层对，如果共享后模型的最终隐藏状态与原始模型的相似度高于阈值 T，则保留该共享；</li>
<li>直到达到目标压缩率 R。</li>
</ol>
</li>
</ul>
<h3 id="6-Liu-等-2024-MiniCache-KV-Cache-Compression-in-Depth-Dimension-for-Large-Language-Models-MiniCache"><a href="#6-Liu-等-2024-MiniCache-KV-Cache-Compression-in-Depth-Dimension-for-Large-Language-Models-MiniCache" class="headerlink" title="6. Liu 等 - 2024 - MiniCache: KV Cache Compression in Depth Dimension for Large Language Models (MiniCache)"></a>6. Liu 等 - 2024 - MiniCache: KV Cache Compression in Depth Dimension for Large Language Models (MiniCache)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>问题: 与 KVSharer 相同，探索无需训练的层间（深度维度）压缩。</li>
<li>发现 (与 KVSharer 对比): MiniCache 观察到相邻层之间（特别是在模型的中后部）的 KV Cache 状态具有高度相似性。</li>
<li>方法: 提出 MiniCache，利用这种相似性来合并相邻层的 KV Cache。</li>
<li>合并策略: 为了精确合并，将 KV 状态向量解耦为幅度和方向。使用球面线性插值 (SLERP) 来合并方向，同时保留各自的幅度。</li>
<li>Token 保留: 引入一个 token 保留策略，识别并单独存储少数差异显著、不适合合并的 outlier 状态对，以最小化性能损失。</li>
</ul>
<h2 id="二、KV-Cache-卸载-Offloading-与状态恢复"><a href="#二、KV-Cache-卸载-Offloading-与状态恢复" class="headerlink" title="二、KV Cache 卸载 (Offloading) 与状态恢复"></a>二、KV Cache 卸载 (Offloading) 与状态恢复</h2><p>该领域论文专注于解决 GPU 显存不足的问题，通过将 KV Cache 卸载到 CPU 内存或磁盘，并研究如何最小化由此产生的 I&#x2F;O 瓶颈。</p>
<h3 id="1-Sheng-等-2023-FlexGen-High-Throughput-Generative-Inference-of-Large-Language-Models-with-a-Single-GPU-FlexGen"><a href="#1-Sheng-等-2023-FlexGen-High-Throughput-Generative-Inference-of-Large-Language-Models-with-a-Single-GPU-FlexGen" class="headerlink" title="1. Sheng 等 - 2023 - FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU (FlexGen)"></a>1. Sheng 等 - 2023 - FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU (FlexGen)</h3><ul>
<li>问题: 如何在资源受限（如单个 GPU）的硬件上实现高吞吐量的 LLM 推理。</li>
<li>方法: 提出 FlexGen，一个高吞吐量推理引擎，它聚合 GPU、CPU 和磁盘的内存与计算资源。</li>
<li>创新点:<ul>
<li>卸载: 核心思想是将模型权重、KV Cache 和激活值卸载 (offload) 到 CPU 内存和磁盘。</li>
<li>优化调度: 将卸载策略形式化为一个搜索空间，并使用线性规划 (Linear Programming) 求解，以在硬件约束下找到最优的张量放置和访问模式，从而最大化吞吐量。</li>
<li>Zig-zag 调度: 提出了 “zig-zag block schedule”（一种列-行混合遍历），在计算一个层时，为多个批次 (batch) 服务，从而分摊权重加载的 I&#x2F;O 开销。</li>
<li>压缩: 结合了 4 位分组量化 (group-wise quantization) 来压缩权重和 KV Cache，进一步减少 I&#x2F;O。</li>
</ul>
</li>
</ul>
<h3 id="2-Gao-等-2024-Cost-Efficient-Large-Language-Model-Serving-for-Multi-turn-Conversations-with-CachedAttention-CachedAttention-AttentionStore"><a href="#2-Gao-等-2024-Cost-Efficient-Large-Language-Model-Serving-for-Multi-turn-Conversations-with-CachedAttention-CachedAttention-AttentionStore" class="headerlink" title="2. Gao 等 - 2024 - Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention (CachedAttention &#x2F; AttentionStore)"></a>2. Gao 等 - 2024 - Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention (CachedAttention &#x2F; AttentionStore)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (FlexGen): FlexGen 提供了通用的卸载框架，而 CachedAttention 专注于解决多轮对话场景下的特定问题：即历史 KV Cache 的重复计算。</li>
<li>方法: 提出 AttentionStore，一个分层（主机内存 + 磁盘）的 KV Cache 缓存系统。当对话会话非活跃时，其 KV Cache 被卸载到 AttentionStore 中保存，而不是丢弃。</li>
<li>I&#x2F;O 隐藏: 为了解决从慢速介质加载 KV Cache 的延迟（即 Challenge 1），系统采用了两种重叠技术：<ol>
<li>层级预加载 (Layer-wise Pre-loading): 在 GPU 计算当前层时，异步从主机内存预加载下一层的 KV Cache。</li>
<li>异步保存 (Asynchronous Saving): 在 GPU 计算时，异步地将 KV Cache 写回到主机内存。</li>
</ol>
</li>
<li>缓存放置: 利用调度器队列(Job queue)中的未来信息，实现了“调度器感知的预取” (scheduler-aware fetching) 和“调度器感知的驱逐” (scheduler-aware eviction) 策略，以智能地在 DRAM 和磁盘间移动缓存。</li>
<li>上下文截断: 通过解耦位置编码 (decoupling the positional encoding)，使得即使在上下文窗口溢出并截断部分 KV Cache 后，剩余的 KV Cache 依然有效。</li>
</ul>
<h3 id="3-Hu-等-2025-TightLLM-Maximizing-Throughput-for-LLM-Inference-via-Adaptive-Offloading-Policy-TightLLM"><a href="#3-Hu-等-2025-TightLLM-Maximizing-Throughput-for-LLM-Inference-via-Adaptive-Offloading-Policy-TightLLM" class="headerlink" title="3. Hu 等 - 2025 - TightLLM: Maximizing Throughput for LLM Inference via Adaptive Offloading Policy (TightLLM)"></a>3. Hu 等 - 2025 - TightLLM: Maximizing Throughput for LLM Inference via Adaptive Offloading Policy (TightLLM)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (FlexGen): FlexGen 这类系统使用静态流水线策略，无法适应推理过程中动态变化的瓶颈。</li>
<li>问题 1 (动态瓶颈): 在解码 (decoding) 阶段，KV Cache 随序列增长而变大，其传输开销逐渐超过计算时间，导致 GPU 空闲。</li>
<li>问题 2 (权重瓶颈): 模型权重的传输开销巨大，难以被单个批次的计算完全掩盖。</li>
<li>方法 1 (KV Distributor): 针对问题1，提出“以计算换传输” (trade-compute-for-transfer) 策略。系统动态决定是加载 KV Cache 的一部分，还是在 GPU 上重计算它。该决策基于一个数学模型（ILP），目的是平衡计算和传输时间，最小化 GPU 空闲。</li>
<li>方法 2 (Weight Loader): 针对问题2，将模型权重切片 (slice)，并将权重加载过程分摊到多个批次的计算过程中，从而隐藏权重传输延迟。</li>
</ul>
<h3 id="4-Jiang-等-2025-KVPR-Efficient-LLM-Inference-with-IO-Aware-KV-Cache-Partial-Recomputation-KVPR"><a href="#4-Jiang-等-2025-KVPR-Efficient-LLM-Inference-with-IO-Aware-KV-Cache-Partial-Recomputation-KVPR" class="headerlink" title="4. Jiang 等 - 2025 - KVPR: Efficient LLM Inference with IO-Aware KV Cache Partial Recomputation (KVPR)"></a>4. Jiang 等 - 2025 - KVPR: Efficient LLM Inference with IO-Aware KV Cache Partial Recomputation (KVPR)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (FlexGen): FlexGen 试图重叠整个 KV Cache 的 I&#x2F;O 和另一层的计算。但这在 I&#x2F;O 远大于计算时效率低下。</li>
<li>核心思想 (KVPR): 提出一种新的重叠方式。CPU 先传输一小部分用于重计算 KV Cache 前段所需的激活值。</li>
<li>方法: GPU 利用这些激活值重计算 KV Cache 的前段部分，与此同时，CPU 通过 PCIe 异步传输 KV Cache 的剩余部分。最后 GPU 合并重计算的部分和传输的部分。</li>
<li>最优分割点: 系统通过 Profiler 收集硬件信息，并由 Scheduler（调度器）解一个线性规划问题 (Linear Programming)，来自动确定“重计算”和“传输”的最佳分割点，以实现最大化重叠。</li>
<li>权重优化: 采用了细粒度流水线，优先加载重计算 KV Cache 所需的 W_K 和 W_V 权重，以便尽早开始重计算。</li>
</ul>
<h3 id="5-Gao-等-2025-Fast-State-Restoration-in-LLM-Serving-with-HCache-HCache"><a href="#5-Gao-等-2025-Fast-State-Restoration-in-LLM-Serving-with-HCache-HCache" class="headerlink" title="5. Gao 等 - 2025 - Fast State Restoration in LLM Serving with HCache (HCache)"></a>5. Gao 等 - 2025 - Fast State Restoration in LLM Serving with HCache (HCache)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (Token Recomputation vs KV Offload): 状态恢复的两种主流方法都有巨大开销：(1) Token 重计算（如 vLLM）受限于 GPU 计算；(2) KV 卸载（如 AttentionStore, FlexGen）受限于 I&#x2F;O 传输。</li>
<li>核心思想 (HCache): 提出从中间激活（即隐藏状态, hidden states）恢复 LLM 状态。</li>
<li>优势: <ol>
<li>I&#x2F;O 方面：隐藏状态的大小约是 KV Cache 的一半（H 和 K+V），I&#x2F;O 传输时间减少 2 倍。</li>
<li>计算方面：从隐藏状态重计算 KV Cache 仅需一次投影操作，跳过了昂贵的 Attention 和 FFN 模块，计算成本降低 6 倍以上。</li>
</ol>
</li>
<li>无气泡调度器: 设计了一个“无气泡恢复调度器” (bubble-free restoration scheduler)。它通过离线分析硬件性能（计算&#x2F;IO速度），动态地将恢复任务在 HCache、KV Offload 和 Token Recomputation 三种方法间进行分层混合，以确保计算和 I&#x2F;O 流水线被完全填满，消除气泡。</li>
<li>存储管理: 解决了隐藏状态保存（逐层）和恢复（逐 token 批次）时的数据布局不匹配问题 (C2)，方法是采用基于块 (chunk-based) 的存储格式和两阶段保存策略。</li>
</ul>
<h3 id="6-Qin-等-2025-MOONCAKE-Trading-More-Storage-for-Less-Computation-–-A-KVCache-centric-Architecture-for-Serving-LLM-MOONCAKE"><a href="#6-Qin-等-2025-MOONCAKE-Trading-More-Storage-for-Less-Computation-–-A-KVCache-centric-Architecture-for-Serving-LLM-MOONCAKE" class="headerlink" title="6. Qin 等 - 2025 - MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM (MOONCAKE)"></a>6. Qin 等 - 2025 - MOONCAKE: Trading More Storage for Less Computation – A KVCache-centric Architecture for Serving LLM (MOONCAKE)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (vLLM, FlexGen): vLLM 虽解决了碎片化，但其分离 Prefill 和 Decode (P&#x2F;D) 的必要性存疑。FlexGen 虽使用卸载，但依赖本地资源。MOONCAKE 旨在通过彻底的架构解耦和全局资源池化来最大化长上下文场景下符合 SLO 的吞吐量。</li>
<li>核心架构: 提出一个以 KV Cache 为中心的深度解耦架构。</li>
<li>MOONCAKE Store: 聚合了 GPU 集群中所有节点的 CPU、DRAM、SSD 和 RDMA NIC 资源，构建了一个 PB 级的、全局的、分层的分布式 KV Cache 池。这是“以存储换计算”理念的大规模实现。</li>
<li>Conductor (全局调度器): 这是一个“缓存感知” (Cache-aware) 的全局调度器。它根据全局缓存池中 KV Cache 的分布情况来调度请求，以最大化 Prefill 阶段的缓存复用（减少计算），同时平衡 Decoding 阶段的负载以满足 TBT SLO。</li>
<li>热点缓存均衡: 包含一个基于启发式的自动热点缓存迁移和复制方案，当检测到拥塞时，将热点 KV 块复制到多个节点。</li>
<li>长上下文 Prefill: 针对长上下文 Prefill 导致的 TTFT 过高问题，实现了分块流水线并行 (Chunked Pipeline Parallelism, CPP)，将单个长请求的 Prefill 任务分布到多个节点上执行，以此降低 TTFT。</li>
</ul>
<h2 id="三、Attention-level-与-内存管理创新"><a href="#三、Attention-level-与-内存管理创新" class="headerlink" title="三、Attention-level 与 内存管理创新"></a>三、Attention-level 与 内存管理创新</h2><p>该领域论文不局限于压缩或卸载，而是重新设计了 Attention 机制本身或其底层的内存管理范式，以解决内存效率和性能问题。</p>
<h3 id="1-Kwon-等-2023-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention-vLLM-PagedAttention"><a href="#1-Kwon-等-2023-Efficient-Memory-Management-for-Large-Language-Model-Serving-with-PagedAttention-vLLM-PagedAttention" class="headerlink" title="1. Kwon 等 - 2023 - Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM &#x2F; PagedAttention)"></a>1. Kwon 等 - 2023 - Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM &#x2F; PagedAttention)</h3><ul>
<li>问题: 传统 LLM 服务系统使用连续内存块来存储 KV Cache，但这导致了严重的内部碎片（为最大长度预留空间）和外部碎片（内存中难以找到足够大的连续块），极大地限制了批处理大小和吞吐量。</li>
<li>方法: 提出 PagedAttention 算法，灵感来源于操作系统的虚拟内存和分页机制。</li>
<li>创新点:<ul>
<li>非连续存储: PagedAttention 将 KV Cache 分割成固定大小的“块” (block)，这些块可以存储在非连续的物理内存中。</li>
<li>消除碎片: 通过按需分配小块内存，基本消除了内部碎片（浪费仅限于最后一个块）和外部碎片（所有块大小相同）。</li>
<li>内存共享: 这种块状管理使得灵活的内存共享成为可能。vLLM 实现了“写时复制” (Copy-on-Write) 机制，允许不同序列（例如并行采样或束搜索）共享它们共同的前缀 KV 块，仅在需要修改时才复制。</li>
</ul>
</li>
</ul>
<h3 id="2-Ye-等-2024-ChunkAttention-Efficient-Self-Attention-with-Prefix-Aware-KV-Cache-and-Two-Phase-Partition-ChunkAttention"><a href="#2-Ye-等-2024-ChunkAttention-Efficient-Self-Attention-with-Prefix-Aware-KV-Cache-and-Two-Phase-Partition-ChunkAttention" class="headerlink" title="2. Ye 等 - 2024 - ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition (ChunkAttention)"></a>2. Ye 等 - 2024 - ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition (ChunkAttention)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (vLLM): vLLM (PagedAttention) 提到了共享 KV Cache（如共享前缀）的可能性，但其实现通常是静态的（即服务提供商需预定义哪些是共享前缀）。</li>
<li>动态前缀共享 (PAKV): 提出 Prefix-Aware KV Cache (PAKV)，将 KV Cache 组织成一个前缀树 (prefix tree) 结构。这使得系统可以在运行时 (runtime) 自动检测并合并具有相同 token 前缀的请求的 KV 缓存，无需预定义。</li>
<li>两阶段分区 (TPP) 核函数: 为这种树状共享缓存设计了专门的 CUDA 核函数 (TPP)。该核函数分为两阶段执行：<ol>
<li>Chunk-first 阶段: 批量处理来自不同序列、但访问相同共享块 (shared chunks) 的查询。</li>
<li>Sequence-first 阶段: 处理剩余的、各个序列独有 (non-shared) 的 KV 块。</li>
</ol>
</li>
<li>优势: TPP 通过在 Chunk-first 阶段将共享块的计算批量化，提高了数据局部性和计算效率（例如，将矢量-矩阵乘法提升为矩阵-矩阵乘法）。</li>
</ul>
<h3 id="3-Prabhu-等-2025-vAttention-Dynamic-Memory-Management-for-Serving-LLMs-without-PagedAttention-vAttention"><a href="#3-Prabhu-等-2025-vAttention-Dynamic-Memory-Management-for-Serving-LLMs-without-PagedAttention-vAttention" class="headerlink" title="3. Prabhu 等 - 2025 - vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention (vAttention)"></a>3. Prabhu 等 - 2025 - vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention (vAttention)</h3><p><strong>创新点与方法</strong>:</p>
<ul>
<li>对比与改进 (vLLM&#x2F;PagedAttention): PagedAttention (vLLM) 解决了物理内存碎片问题，但代价是使 KV Cache 在虚拟内存中变得非连续。这带来了巨大的编程和维护负担（必须重写所有注意力核函数）和性能开销（PagedAttention 核函数通常比非分页核函数慢）。</li>
<li>核心思想 (vAttention): vAttention 旨在同时实现物理内存的动态分配和虚拟内存的连续性。</li>
<li>方法: 利用现代 CUDA 提供的虚拟内存管理 (VMM) APIs，vAttention 首先为 KV Cache 预留 (reserve) 一个大的、连续的虚拟地址空间（这不消耗物理内存）。</li>
<li>按需映射: 在运行时（如 prefill 或 decode 阶段），vAttention 才按需分配物理内存页，并使用 cuMemMap 等 API 将这些（可能非连续的）物理页映射到该虚拟地址空间中需要的位置。</li>
<li>优势: 由于 KV Cache 在虚拟地址上是连续的，vAttention 可以直接使用任何标准的高性能注意力核函数（如 FlashAttention-2, FlashAttention-3），无需任何修改。</li>
<li>VMM 优化: 解决了 CUDA VMM API 的局限性：<ol>
<li>通过后台线程和预取隐藏 VMM API 的高延迟；</li>
<li>通过修改 NVIDIA 驱动增加了对 64KB 等小页面的支持，以缓解 2MB 大页面带来的内部碎片问题。</li>
</ol>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" class="category-chain-item">论文阅读</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>KVCache压缩、Offloading、Attention优化论文总结</div>
      <div>http://example.com/2025/11/24/KVCache压缩、Offloading、Attention优化/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>wyz</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年11月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/11/24/go%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/" title="go基础语法">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">go基础语法</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
